
def print_ppo_conf(model):
    print("Training Configuration:")
    print("  Batch Size:", model.batch_size)
    print("  Number of Steps:", model.n_steps)
    print("  Number of Epochs:", model.n_epochs)
    print("  Clip Range:", model.clip_range)
    print("  Entropy Coefficient:", model.ent_coef)
    print("  Learning Rate:", model.learning_rate)
    print("  Gamma (discount factor):", model.gamma)
    print("  GAE Lambda:", model.gae_lambda)
    print("  Max Grad Norm:", model.max_grad_norm)
    print("  Target KL Divergence:", model.target_kl)
    print("  Use SDE (Stochastic Differential Equations):", model.use_sde)
    print("  SDE Sample Frequency:", model.sde_sample_freq)
    print("  Learning Rate Scheduler:", model.lr_schedule)
    # print("  Use Optimizer:", model.optimizer_class)
    # print("  Use Scheduler:", model.scheduler_class)

    print("\nPolicy Configuration:")
    print("  Policy kwargs:", model.policy_kwargs)
    print("  Policy Class:", model.policy_class)


def print_sac_conf(model):
    print("SAC Configuration:")
    print("  Batch Size:", model.batch_size)
    print("  Buffer Size:", model.buffer_size)
    print("  Learning Rate:", model.learning_rate)
    print("  Gamma (discount factor):", model.gamma)
    print("  Tau (soft update coefficient):", model.tau)
    print("  Entropy Coefficient:", model.ent_coef)
    print("  Target Entropy:", model.target_entropy)
    print("  Learning Rate Scheduler:", model.lr_schedule)
    print("  Number of Environments:", model.n_envs)
    print("  Number of Timesteps:", model.num_timesteps)

    print("\nPolicy Configuration:")
    print("  Policy kwargs:", model.policy_kwargs)
    print("  Policy Class:", model.policy_class)

    print("\nInternal State:")
    print("  Episode Number:", model._episode_num)
    print("  Last Observation:", model._last_obs)
    print("  Last Episode Starts:", model._last_episode_starts)
