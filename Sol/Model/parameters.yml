

best_ppo:
  targets:  Waypoints.up_sharp_back_turn()
  n_envs: 12
  n_steps: 4096
  batch_size: 256
  n_epochs: 20
  gamma: 0.99
  vf_coef: 0.5
  gae_lambda: 0.9
  normalize_advantage: True
  clip_range: 0.1
  learning_rate: 0.003
  policy_kwargs:
    activation_fn: Tanh
    share_features_extractor: True
    net_arch:
      vf: [ 256, 256 ]
      pi: [ 256, 256 ]